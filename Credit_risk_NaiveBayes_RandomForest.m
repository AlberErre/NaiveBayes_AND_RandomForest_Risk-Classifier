%% Papaso

close all
clear all
clc

rng('default')

%% Import data
Z = csvread('f_clean_manageable_data_credictcard.csv', 1); % (no headers)

%% Define data
Y = Z(:,end); % output (response)
X = Z(:,1:end-1); % model inputs (predictors)

%% Training and test sets
X_train = X(1:23680,:);
Y_train = Y(1:23680,:);

X_test = X(23681:end,:);
Y_test = Y(23681:end,:);

%% Plots
loan = X(:,1);
histogram(loan) % no enough information to select a density distribution
xlabel('Credit amount (NT Dolars)');

age = X(:,2);
histogram(age) % Clear normal distribution
xlabel('Age');

%% Naive Bayes

%% Define probability distribution for each variable/attribute
distribution_types = string();
distribution_types(1) = 'kernel'; % Best result according to 'OptimizeHyperparameters', kernel, box, Width = 15695
distribution_types(2) = 'normal'; % Justified by the graphical analysis
distribution_types(3:6) = 'mvmn'; % categoritcal attributes with smoothing included
distribution_types = cellstr(distribution_types);

%% Define categorical variables/attributes
categorical_variables = [3 4 5 6]; 
        % This refers the columns where attributes are categorical

%% Prior probabilities
prior = [0.78 0.22];% Obtained from the probability distribution of the dataset, 78% are paid cases and 22% are default cases

%% Cost matrix (missclassification). IMPORTANT, explained in the report.
miss_matrix = [0 1.5;
               3.5 0]; % The optimal result usually got unbalanced regarding default payments.
                       % Therefore, we apply this matrix to avoid missclasification

%% Naive Bayes Classifier
tic
naive = fitcnb(X_train, Y_train,'DistributionNames',distribution_types, ... % Define the PD of each variable/attribute 
        'CategoricalPredictors',categorical_variables, ... 
        'Kfold',10, ...  % Higher K = best quality of training    
        'Kernel','box','Width',15695,'Support','positive', ... % Best PD according to 'OptimizeHyperparameters'
        'Prior',prior, ...   
        'Cost',miss_matrix);
toc
naive

%% Training error
training_error_kfold = kfoldLoss(naive, 'mode', 'individual'); % mean squared error losses (vector)

training_error_average = kfoldLoss(naive, 'mode', 'average'); % average loss (mean squared error)
training_error_average

% Select the best trained model using kfold (min error) to predict
[training_error, min_error_kfold] = min(training_error_kfold);
[training_error, min_error_kfold]

%% Validation error
% Predict new values using the best trained kfold (MIN training error)
Y_predict = predict(naive.Trained{min_error_kfold}, X_test);
c_matrix = confusionmat(Y_test,Y_predict); 
c_matrix % Confusion matrix is used to see how well our model predicts according to our objective (default credit card)

%% Accurancy Level
% SUM of correctly predicted cases regarding total cases
model_accurancy = sum(Y_predict == Y_test)/length(Y_test) * 100;
model_accurancy
test_error = 100 - model_accurancy;
test_error

%% Random Forest

%% Random Forest Hyperparamter optimisation 

%Optimising for number of trees and number of features (variables) to sample
Ntrees = 200

% Random forest - 200 trees - 1 feature
% note: the paramters inside tree bagger are all explained below
Mdl_1 = TreeBagger(Ntrees,X_train,Y_train,... 
                 'NumPredictorsToSample', 1, ...
                 'InBagFraction', 1, ...
                 'Method', 'classification',...
                 'OOBPrediction', 'On', ...
                 'oobvarimp', 'on', ...
                 'CategoricalPredictors', categorical_variables, ...
                 'MinLeafSize', 1, ...
                 'Cost',miss_matrix);
             
Error_1 = oobError(Mdl_1);

% Random forest - 200 trees - 2 features
Mdl_2 = TreeBagger(Ntrees,X_train,Y_train,... 
                 'NumPredictorsToSample', 2, ...
                 'InBagFraction', 1, ...
                 'Method', 'classification',...
                 'OOBPrediction', 'On', ...
                 'oobvarimp', 'on', ...
                 'CategoricalPredictors', categorical_variables, ...
                 'MinLeafSize', 1, ...
                 'Cost',miss_matrix);
             
Error_2 = oobError(Mdl_2);

% Random forest - 200 trees - 3 features
Mdl_3 = TreeBagger(Ntrees,X_train,Y_train,... 
                 'NumPredictorsToSample', 3, ...
                 'InBagFraction', 1, ...
                 'Method', 'classification',...
                 'OOBPrediction', 'On', ...
                 'oobvarimp', 'on', ...
                 'CategoricalPredictors', categorical_variables, ...
                 'MinLeafSize', 1, ...
                 'Cost',miss_matrix);
             
Error_3 = oobError(Mdl_3);

% Random forest - 200 trees - 4 features
Mdl_4 = TreeBagger(Ntrees,X_train,Y_train,... 
                 'NumPredictorsToSample', 4, ...
                 'InBagFraction', 1, ...
                 'Method', 'classification',...
                 'OOBPrediction', 'On', ...
                 'oobvarimp', 'on', ...
                 'CategoricalPredictors', categorical_variables, ...
                 'MinLeafSize', 1, ...
                 'Cost',miss_matrix);
             
Error_4 = oobError(Mdl_4);

% Random forest - 200 trees - 5 features
Mdl_5 = TreeBagger(Ntrees,X_train,Y_train,... 
                 'NumPredictorsToSample', 5, ...
                 'InBagFraction', 1, ...
                 'Method', 'classification',...
                 'OOBPrediction', 'On', ...
                 'oobvarimp', 'on', ...
                 'CategoricalPredictors', categorical_variables, ...
                 'MinLeafSize', 1, ...
                 'Cost',miss_matrix);
             
Error_5 = oobError(Mdl_5);


Error_table = [Error_1 Error_2 Error_3 Error_4 Error_5];

figure;
surf(Error_table) %plots missclassification error vs number of trees vs number of predictors to sample
xlabel 'Number of predictors to sample';
ylabel  'Number of grown trees';
zlabel 'Out-of-bag classification error';

%% Hyperparameter: plot minimum leaf size vs misclassification error

% Random forest - Minleaf size: 1 (default)
Mdl_leaf_1 = TreeBagger(50 ,X_train,Y_train,... 
                 'NumPredictorsToSample', 2, ...
                 'InBagFraction', 0.8, ...
                 'Method', 'classification',...
                 'OOBPrediction', 'On', ...
                 'CategoricalPredictors', categorical_variables, ...
                 'MinLeafSize', 1, ...
                 'Cost',miss_matrix);
             
Error_leaf_1 = oobError(Mdl_leaf_1); %missclassification rate

% Random forest - Minleaf size: 5
Mdl_leaf_2 = TreeBagger(50 ,X_train,Y_train,... 
                 'NumPredictorsToSample', 2, ...
                 'InBagFraction', 0.8, ...
                 'Method', 'classification',...
                 'OOBPrediction', 'On', ...
                 'CategoricalPredictors', categorical_variables, ...
                 'MinLeafSize', 5, ...
                 'Cost',miss_matrix);
             
Error_leaf_2 = oobError(Mdl_leaf_2);

% Random forest - Minleaf size: 15
Mdl_leaf_3 = TreeBagger(50 ,X_train,Y_train,... 
                 'NumPredictorsToSample', 2, ...
                 'InBagFraction', 0.8, ...
                 'Method', 'classification',...
                 'OOBPrediction', 'On', ...
                 'CategoricalPredictors', categorical_variables, ...
                 'MinLeafSize', 15, ...
                 'Cost',miss_matrix);
             
Error_leaf_3 = oobError(Mdl_leaf_3);

% Random forest - Minleaf size: 25
Mdl_leaf_4 = TreeBagger(50 ,X_train,Y_train,... 
                 'NumPredictorsToSample', 2, ...
                 'InBagFraction', 0.8, ...
                 'Method', 'classification',...
                 'OOBPrediction', 'On', ...
                 'CategoricalPredictors', categorical_variables, ...
                 'MinLeafSize', 25, ...
                 'Cost',miss_matrix);
          
Error_leaf_4 = oobError(Mdl_leaf_4);

% Random forest - Minleaf size: 50
Mdl_leaf_5 = TreeBagger(50 ,X_train,Y_train,... 
                 'NumPredictorsToSample', 2, ...
                 'InBagFraction', 0.8, ...
                 'Method', 'classification',...
                 'OOBPrediction', 'On', ...
                 'CategoricalPredictors', categorical_variables, ...
                 'MinLeafSize', 50, ...
                 'Cost',miss_matrix);
          
Error_leaf_5 = oobError(Mdl_leaf_5);

% Random forest - Minleaf size: 100
Mdl_leaf_6 = TreeBagger(50 ,X_train,Y_train,... 
                 'NumPredictorsToSample', 2, ...
                 'InBagFraction', 0.8, ...
                 'Method', 'classification',...
                 'OOBPrediction', 'On', ...
                 'CategoricalPredictors', categorical_variables, ...
                 'MinLeafSize', 100, ...
                 'Cost',miss_matrix);
          
Error_leaf_6 = oobError(Mdl_leaf_6);

% Random forest - Minleaf size: 300
Mdl_leaf_7 = TreeBagger(50 ,X_train,Y_train,... 
                 'NumPredictorsToSample', 2, ...
                 'InBagFraction', 0.8, ...
                 'Method', 'classification',...
                 'OOBPrediction', 'On', ...
                 'CategoricalPredictors', categorical_variables, ...
                 'MinLeafSize', 300, ...
                 'Cost',miss_matrix);
          
Error_leaf_7 = oobError(Mdl_leaf_7);

% Random forest - Minleaf size: 800
Mdl_leaf_8 = TreeBagger(50 ,X_train,Y_train,... 
                 'NumPredictorsToSample', 2, ...
                 'InBagFraction', 0.8, ...
                 'Method', 'classification',...
                 'OOBPrediction', 'On', ...
                 'CategoricalPredictors', categorical_variables, ...
                 'MinLeafSize', 800, ...
                 'Cost',miss_matrix);
          
Error_leaf_8 = oobError(Mdl_leaf_8);



Error_table_leaf = [Error_leaf_1 Error_leaf_2 Error_leaf_3 Error_leaf_4 Error_leaf_5 Error_leaf_6 Error_leaf_7 Error_leaf_8];

figure;
surf(Error_table_leaf)
xlabel 'Minimum Leaf Size';
ylabel  'Number of grown trees';
zlabel 'Out-of-bag classification error';

%% More hyperparameters: plot minimum leaf size vs computational time

for j = 1:500:3000
      Mdl = TreeBagger(50 ,X_train,Y_train,... 
                 'NumPredictorsToSample', 2, ...
                 'InBagFraction', 1, ...
                 'Method', 'classification',...
                 'CategoricalPredictors', categorical_variables, ...
                 'MinLeafSize', j, ...
                 'Cost',miss_matrix);
    prediction = Mdl.predict(X_test);
    forest_model_accuracy(j) = sum(str2double(prediction) == Y_test)/length(Y_test) * 100;
end

D = forest_model_accuracy(forest_model_accuracy~=0); %remove 0s from D

plot(D) %plots minimum leaf size vs computational time

%% Train the random forest using appropriate parameters 

%Train the random forest using 50 trees and 2 predictors to sample
Mdl = TreeBagger(50,X_train,Y_train,... 
                 'InBagFraction', 0.8, ... %use random 80% of train data on each tree
                 'Method', 'classification',... %Classification Problem
                 'NumPredictorsToSample', 2, ... %2 predictors to sample
                 'OOBPrediction', 'On', ... %used for missclassification error
                 'CategoricalPredictors', categorical_variables, ... %specifies which variables are categorical
                 'MinLeafSize', 1); %specify the minimum leaf size
             

%% Apply Random Forest to test data 
prediction = Mdl.predict(X_test);

%% Confusion matrix
[C, order] = confusionmat(Y_test, str2double(prediction))

%% Predictor importance for Random Forest %NOTE works in matlab 2016b if you add 'oobvarimp', 'on' as inputs to Treebagger line 298
%weights = Mdl.OOBPermutedVarDeltaError; 
%[B, iranked] = sort(weights, 'descend');
%barh(weights(iranked))

%% Accuracy of the Random Forest
forest_model_accuracy = sum(str2double(prediction) == Y_test)/length(Y_test) * 100;
forest_model_accuracy

%% Plot how long it takes to train a model based on number of trees

for i = 1:10:200
    tic
    Mdl = TreeBagger(i,X_train,Y_train,... 
                 'InBagFraction', 0.8, ...
                 'Method', 'classification',...
                 'NumPredictorsToSample', 2, ...
                 'OOBPrediction', 'On', ...
                 'CategoricalPredictors', categorical_variables, ...
                 'MinLeafSize', 1,...
                 'cost', miss_matrix);
    t(i) = toc
end
t(t==0) = []; %for loop produces a lot of 0s we want to remove before plotting
figure
xlabel('Number of Trees');
ylabel('Time taken');
plot(t);